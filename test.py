# --- Imports ---
import os
import sys
import pandas as pd
from collections import Counter
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import recall_score, make_scorer, accuracy_score, classification_report, precision_recall_curve, ConfusionMatrixDisplay, roc_auc_score
from sklearn.model_selection import train_test_split, GridSearchCV
import joblib
import matplotlib.pyplot as plt

# --- Ajout du chemin vers utils.py ---
sys.path.append(os.path.abspath(".."))
from utils import execute_model, save_results, calculate_scores, ThresholdedClassifier



# --- Chargement des donn√©es ---
DF_PATH = "../data/balanced_3.csv"
df = pd.read_csv(DF_PATH, sep=";")

# --- S√©paration features / cible ---
X = df.drop(columns=["cardio"])
y = df["cardio"]

# --- Calcul poids manuel pour d√©s√©quilibre ---
counter = Counter(y)
n0, n1 = counter[0], counter[1]
manual_weight = {0: 1, 1: round(n0 / n1, 2)}
print("Poids manuel calcul√© :", manual_weight)

# --- Cr√©ation d'un scorer personnalis√© (recall classe 1) ---
recall_scorer = make_scorer(recall_score, pos_label=1)

# --- D√©finition mod√®le de base ---
model = LogisticRegression()

# --- Split train/test ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)


# --- Grille d'hyperparam√®tres pour GridSearchCV ---
param_grid = [
    {
        'penalty': ['elasticnet'],
        'solver': ['saga'],
        'C': [28, 29],
        'l1_ratio': [0.47, 0.49],
        'class_weight': ['balanced', manual_weight],
        'max_iter': [1000]
    }
]


# --- GridSearchCV ---
grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring=recall_scorer,
    n_jobs=-1,
    verbose=2
)

if "best_model_grid" not in globals():
    grid_search.fit(X_train, y_train)
    best_model_grid = grid_search.best_estimator_
    print("‚úÖ Grid search termin√©e.")
else:
    print("‚úÖ Mod√®le d√©j√† entra√Æn√©, utilisation de best_model_grid.")

print("Meilleurs hyperparam√®tres :", grid_search.best_params_)
print("Meilleur score CV (recall) :", grid_search.best_score_)


# --- Sauvegarde ou Chargement du meilleur mod√®le ---
MODEL_PATH = "best_model_logreg.joblib"

if not os.path.exists(MODEL_PATH):
    joblib.dump(best_model_grid, MODEL_PATH)
    print("‚úÖ Mod√®le sauvegard√©.")
else:
    best_model_grid = joblib.load(MODEL_PATH)
    print("üì¶ Mod√®le charg√© depuis le disque.")

# --- Obtenir les probabilit√©s sur le test set ---
y_proba = best_model_grid.predict_proba(X_test)[:, 1]

# --- Trouver le meilleur seuil ---
def find_best_threshold_for_recall(y_true, y_proba):
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    best_index = recall.argmax()
    best_threshold = thresholds[best_index - 1] if best_index > 0 else 0.5
    print(f"üîç Meilleur seuil trouv√© : {best_threshold:.4f} avec recall = {recall[best_index]:.4f}")
    return best_threshold

best_threshold = find_best_threshold_for_recall(y_test, y_proba)

precision, recall, thresholds = precision_recall_curve(y_test, y_proba)



# --- Mod√®le envelopp√© avec seuil ---
final_model = ThresholdedClassifier(model=best_model_grid, threshold=best_threshold)

# --- Pr√©dictions avec seuil personnalis√© ---
y_pred = final_model.predict(X_test)
y_proba_full = final_model.predict_proba(X_test)

# --- Scores ---
scores = calculate_scores(y_test, y_pred, y_proba=y_proba_full[:, 1], prefix="metric_")
scores["threshold"] = best_threshold

# --- Enregistrement des r√©sultats ---
save_results(
    model=best_model_grid,
    scores=scores,
    data_path=DF_PATH,
    output_path="../results_2.csv"
)

# --- (Optionnel) Affichage de la matrice de confusion ---
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.title("Matrice de confusion avec seuil personnalis√©")
plt.show()


plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall curve')
plt.show()

plt.plot(thresholds, precision[:-1], label='Pr√©cision')
plt.plot(thresholds, recall[:-1], label='Rappel')
plt.xlabel('Seuil de d√©cision')
plt.ylabel('Score')
plt.legend()
plt.title('Courbe Pr√©cision-Rappel selon le seuil')
plt.grid()
plt.show()

ConfusionMatrixDisplay.from_predictions(y_test, y_pred)



def find_best_threshold_for_recall(y_true, y_proba):
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    
    # On veut maximiser le recall
    best_index = recall.argmax()
    best_threshold = thresholds[best_index - 1] if best_index > 0 else 0.5
    
    print(f"Meilleur seuil trouv√© : {best_threshold:.4f} avec recall = {recall[best_index]:.4f}")
    return best_threshold

# Exemple d‚Äôutilisation
y_proba = best_model_grid.predict_proba(X_test)[:, 1]
best_threshold = find_best_threshold_for_recall(y_test, y_proba)

# Pr√©diction avec ce seuil personnalis√©
y_pred_custom = (y_proba >= best_threshold).astype(int)

# √âvaluation avec ce nouveau seuil
custom_recall = recall_score(y_test, y_pred_custom)
print(f"Recall avec seuil personnalis√© : {custom_recall:.4f}")


def find_best_threshold_for_recall(y_true, y_proba):
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    
    # On veut maximiser le recall
    best_index = recall.argmax()
    best_threshold = thresholds[best_index - 1] if best_index > 0 else 0.5
    
    print(f"Meilleur seuil trouv√© : {best_threshold:.4f} avec recall = {recall[best_index]:.4f}")
    return best_threshold

# Exemple d‚Äôutilisation
y_proba = best_model_grid.predict_proba(X_test)[:, 1]
best_threshold = find_best_threshold_for_recall(y_test, y_proba)

# Pr√©diction avec ce seuil personnalis√©
y_pred_custom = (y_proba >= best_threshold).astype(int)

# √âvaluation avec ce nouveau seuil
custom_recall = recall_score(y_test, y_pred_custom)
print(f"Recall avec seuil personnalis√© : {custom_recall:.4f}")
